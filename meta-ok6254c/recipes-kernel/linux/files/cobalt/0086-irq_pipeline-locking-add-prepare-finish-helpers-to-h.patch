From bf855d7ff25bbaa6912804b12658a3db453b28c6 Mon Sep 17 00:00:00 2001
From: Philippe Gerum <rpm@xenomai.org>
Date: Wed, 30 Dec 2020 19:25:44 +0100
Subject: [PATCH 086/179] irq_pipeline: locking: add prepare, finish helpers to
 hard spinlocks

The companion core may make good use of a way to act upon a locking
operation which is about to start, or an unlocking operation which has
just taken place. Typically, some debug code could be enabled this
way, checking for the consistency of such operations. Since hybrid
spinlocks are based on hard spinlocks, those helpers are available in
both cases.

The locking process is now as follows:

IRQ forms:

* locking:     hard_disable_irqs + lock_prepare + spin_on_lock
* try-locking: hard_disable_irqs + trylock_prepare + try_lock, trylock_fail if busy
* unlocking:   unlock + lock_finish + hard_enable_irqs

basic forms:

* locking:     lock_prepare + spin_on_lock
* try-locking: trylock_prepare + try_lock, trylock_fail if busy
* unlocking:   unlock + lock_finish

hard_spin_lock_prepare() and hard_spin_unlock_finish() are such
helpers. An empty implementation is provided by
include/dovetail/spinlock.h, which the core may override.

Signed-off-by: Philippe Gerum <rpm@xenomai.org>
---
 include/dovetail/spinlock.h       | 21 ++++++++++++++++++++
 include/linux/spinlock_pipeline.h | 32 ++++++++++++++++++++++++-------
 kernel/locking/pipeline.c         |  5 +++++
 3 files changed, 51 insertions(+), 7 deletions(-)
 create mode 100644 include/dovetail/spinlock.h

diff --git a/include/dovetail/spinlock.h b/include/dovetail/spinlock.h
new file mode 100644
index 000000000..381031afd
--- /dev/null
+++ b/include/dovetail/spinlock.h
@@ -0,0 +1,21 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _DOVETAIL_SPINLOCK_H
+#define _DOVETAIL_SPINLOCK_H
+
+/* Placeholders for hard/hybrid spinlock modifiers. */
+
+struct raw_spinlock;
+
+static inline void hard_spin_lock_prepare(struct raw_spinlock *lock)
+{ }
+
+static inline void hard_spin_unlock_finish(struct raw_spinlock *lock)
+{ }
+
+static inline void hard_spin_trylock_prepare(struct raw_spinlock *lock)
+{ }
+
+static inline void hard_spin_trylock_fail(struct raw_spinlock *lock)
+{ }
+
+#endif /* !_DOVETAIL_SPINLOCK_H */
diff --git a/include/linux/spinlock_pipeline.h b/include/linux/spinlock_pipeline.h
index 6407bc44e..875cecb70 100644
--- a/include/linux/spinlock_pipeline.h
+++ b/include/linux/spinlock_pipeline.h
@@ -10,6 +10,8 @@
 # error "Please don't include this file directly. Use spinlock.h."
 #endif
 
+#include <dovetail/spinlock.h>
+
 #define hard_spin_lock_irqsave(__rlock, __flags)		\
 	do {							\
 		(__flags) = __hard_spin_lock_irqsave(__rlock);	\
@@ -31,20 +33,24 @@
  */
 #define hard_lock_acquire(__rlock, __try, __ip)				\
 	do {								\
+		hard_spin_lock_prepare(__rlock);			\
 		if (irq_pipeline_debug_locking()) {			\
 			spin_acquire(&(__rlock)->dep_map, 0, __try, __ip); \
 			LOCK_CONTENDED(__rlock, do_raw_spin_trylock, do_raw_spin_lock); \
-		} else							\
+		} else {						\
 			do_raw_spin_lock(__rlock);			\
+		}							\
 	} while (0)
 
 #define hard_lock_acquire_nested(__rlock, __subclass, __ip)		\
 	do {								\
+		hard_spin_lock_prepare(__rlock);			\
 		if (irq_pipeline_debug_locking()) {			\
 			spin_acquire(&(__rlock)->dep_map, __subclass, 0, __ip); \
 			LOCK_CONTENDED(__rlock, do_raw_spin_trylock, do_raw_spin_lock); \
-		} else							\
+		} else {						\
 			do_raw_spin_lock(__rlock);			\
+		}							\
 	} while (0)
 
 #define hard_trylock_acquire(__rlock, __try, __ip)			\
@@ -58,6 +64,7 @@
 		if (irq_pipeline_debug_locking())			\
 			spin_release(&(__rlock)->dep_map, __ip);	\
 		do_raw_spin_unlock(__rlock);				\
+		hard_spin_unlock_finish(__rlock);			\
 	} while (0)
 
 #if defined(CONFIG_SMP) || defined(CONFIG_DEBUG_SPINLOCK)
@@ -138,10 +145,15 @@ unsigned long __hard_spin_lock_irqsave(struct raw_spinlock *rlock)
 static inline
 int hard_spin_trylock(struct raw_spinlock *rlock)
 {
+	hard_spin_trylock_prepare(rlock);
+
 	if (do_raw_spin_trylock(rlock)) {
 		hard_trylock_acquire(rlock, 1, _THIS_IP_);
 		return 1;
 	}
+
+	hard_spin_trylock_fail(rlock);
+
 	return 0;
 }
 
@@ -213,9 +225,9 @@ int hard_spin_is_contended(struct raw_spinlock *rlock)
  * In the pipeline entry context, the regular preemption and root
  * stall logic do not apply since we may actually have preempted any
  * critical section of the kernel which is protected by regular
- * locking (spin or stall), or we may even have preempted the head
- * stage. Therefore, we just need to grab the raw spinlock underlying
- * a hybrid spinlock to exclude other CPUs.
+ * locking (spin or stall), or we may even have preempted the
+ * out-of-band stage. Therefore, we just need to grab the raw spinlock
+ * underlying a hybrid spinlock to exclude other CPUs.
  *
  * NOTE: When entering the pipeline, IRQs are already hard disabled.
  */
@@ -307,10 +319,12 @@ int __hybrid_spin_trylock(struct raw_spinlock *rlock);
 static inline int hybrid_spin_trylock(struct raw_spinlock *rlock)
 {
 	if (in_pipeline()) {
+		hard_spin_trylock_prepare(rlock);
 		if (do_raw_spin_trylock(rlock)) {
 			hard_trylock_acquire(rlock, 1, _THIS_IP_);
 			return 1;
 		}
+		hard_spin_trylock_fail(rlock);
 		return 0;
 	}
 
@@ -324,13 +338,17 @@ int __hybrid_spin_trylock_irqsave(struct raw_spinlock *rlock,
 	({								\
 		int __ret = 1;						\
 		if (in_pipeline()) {					\
+			hard_spin_trylock_prepare(__rlock);		\
 			if (do_raw_spin_trylock(__rlock)) {		\
 				hard_trylock_acquire(__rlock, 1, _THIS_IP_); \
 				(__flags) = hard_local_save_flags();	\
-			} else						\
+			} else {					\
+				hard_spin_trylock_fail(__rlock);	\
 				__ret = 0;				\
-		} else							\
+			}						\
+		} else {						\
 			__ret = __hybrid_spin_trylock_irqsave(__rlock, &(__flags)); \
+		}							\
 		__ret;							\
 	})
 
diff --git a/kernel/locking/pipeline.c b/kernel/locking/pipeline.c
index ba1b6987d..fde458e82 100644
--- a/kernel/locking/pipeline.c
+++ b/kernel/locking/pipeline.c
@@ -172,12 +172,14 @@ int __hybrid_spin_trylock(struct raw_spinlock *rlock)
 	lock = container_of(rlock, struct hybrid_spinlock, rlock);
 	__flags = hard_local_irq_save();
 
+	hard_spin_trylock_prepare(rlock);
 	if (do_raw_spin_trylock(rlock)) {
 		lock->hwflags = __flags;
 		hard_trylock_acquire(rlock, 1, _RET_IP_);
 		return 1;
 	}
 
+	hard_spin_trylock_fail(rlock);
 	hard_local_irq_restore(__flags);
 
 	if (running_inband())
@@ -205,12 +207,15 @@ int __hybrid_spin_trylock_irqsave(struct raw_spinlock *rlock,
 		preempt_disable();
 	}
 
+	hard_spin_trylock_prepare(rlock);
 	if (do_raw_spin_trylock(rlock)) {
 		hard_trylock_acquire(rlock, 1, _RET_IP_);
 		lock->hwflags = __flags;
 		return 1;
 	}
 
+	hard_spin_trylock_fail(rlock);
+
 	if (inband && !*flags) {
 		trace_hardirqs_on();
 		unstall_inband_nocheck();
-- 
2.38.1

