From bec94bd37c0ede40526de06e85d5618f2b069326 Mon Sep 17 00:00:00 2001
From: Philippe Gerum <rpm@xenomai.org>
Date: Sun, 21 Mar 2021 12:12:37 +0100
Subject: [PATCH 100/179] dovetail: rework address space pinning

Real-time applications contolled by the out-of-band core require some
guarantees regarding how memory is managed for them in order to
prevent unexpected delays:

[1] paging must be disabled, all current and future pages must be
    faulted in.

[2] copy-on-write must not be relied upon between a real-time parent
    and any of its children in order to share pages upon fork(). IOW,
    every child should get its own copy of the parent's pages upon
    fork(), and the latter should NOT have to be marked read-only as a
    result of this.

The former implementation relied on Dovetail-specific code to address
these requirements:

- force_commit_memory() would scan all VMAs attached to the caller's
  address space in order to fault them in via commit_vma(). A new task
  attaching to the out-of-band core was expected to call
  force_commit_memory() in order to process the address space
  accordingly.

- commit_vma() would populate a VMA by calling
  populate_vma_page_range() for common mappings, or pin special
  mappings via GUP such as huge pages.

- commit_vma() would also be called when the protection bits of a page
  is changed, in order to catch cases which would require more
  COW-breaking as a result. This is useless, copy_pte_range() is the
  only code path where pages may have to be unCOWed.

COW-breaking upon fork() was not yet performed by Dovetail.

These applications can use mlockall(MCL_CURRENT|MCL_FUTURE) in order
to enforce [1], this will ensure the mappings attached to the caller's
mm are populated and faulted in when applicable. Locking the memory
has been a requirement for these applications since day
one. Therefore, force_commit_memory() is redundant with
mlockall(MCL_CURRENT).

[2] can be obtained by extending to Dovetail-aware memory the
COW-breaking logic readily available to pinned pages (FOLL_PIN) in
copy_pte_range() -> copy_present_pte() -> copy_present_page(). The
real address space of a task which calls dovetail_init_altsched() can
be marked as Dovetail-aware in the process, since such a call is a
clear hint that the underlying task will require both [1] and [2].

At this chance, MMF_VM_PINNED is renamed MMF_DOVETAILED to fix a
confusing name clash with the page pinning logic, which has different
semantics.

Signed-off-by: Philippe Gerum <rpm@xenomai.org>
---
 include/linux/mm.h             | 11 ------
 include/linux/sched/coredump.h |  2 +-
 kernel/dovetail.c              | 17 ++++++++-
 mm/memory.c                    | 69 +++++-----------------------------
 mm/mprotect.c                  |  8 ----
 5 files changed, 26 insertions(+), 81 deletions(-)

diff --git a/include/linux/mm.h b/include/linux/mm.h
index 8bc1560b5..a3fe843ef 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -3183,17 +3183,6 @@ unsigned long wp_shared_mapping_range(struct address_space *mapping,
 				      pgoff_t first_index, pgoff_t nr);
 #endif
 
-#ifdef CONFIG_DOVETAIL
-int commit_vma(struct mm_struct *mm, struct vm_area_struct *vma);
-int force_commit_memory(void);
-#else
-static inline
-int commit_vma(struct mm_struct *mm, struct vm_area_struct *vma)
-{
-	return 0;
-}
-#endif
-
 extern int sysctl_nr_trim_pages;
 
 /**
diff --git a/include/linux/sched/coredump.h b/include/linux/sched/coredump.h
index 054307ff2..0b0694060 100644
--- a/include/linux/sched/coredump.h
+++ b/include/linux/sched/coredump.h
@@ -74,7 +74,7 @@ static inline int get_dumpable(struct mm_struct *mm)
 #define MMF_OOM_REAP_QUEUED	26	/* mm was queued for oom_reaper */
 #define MMF_MULTIPROCESS	27	/* mm is shared between processes */
 #define MMF_DISABLE_THP_MASK	(1 << MMF_DISABLE_THP)
-#define MMF_VM_PINNED		31	/* disable ondemand memory */
+#define MMF_DOVETAILED		31	/* mm belongs to a dovetailed process */
 
 #define MMF_INIT_MASK		(MMF_DUMPABLE_MASK | MMF_DUMP_FILTER_MASK |\
 				 MMF_DISABLE_THP_MASK)
diff --git a/kernel/dovetail.c b/kernel/dovetail.c
index 0878716ae..013d6ed16 100644
--- a/kernel/dovetail.c
+++ b/kernel/dovetail.c
@@ -28,11 +28,26 @@ void inband_task_init(struct task_struct *p)
 void dovetail_init_altsched(struct dovetail_altsched_context *p)
 {
 	struct task_struct *tsk = current;
+	struct mm_struct *mm = tsk->mm;
 
 	check_inband_stage();
 	p->task = tsk;
-	p->active_mm = tsk->mm;
+	p->active_mm = mm;
 	p->borrowed_mm = false;
+
+	/*
+	 * Make sure the current process will not share any private
+	 * page with its child upon fork(), sparing it the random
+	 * latency induced by COW. MMF_DOVETAILED is never cleared once
+	 * set. We serialize with dup_mmap() which holds the mm write
+	 * lock.
+	 */
+	if (!(tsk->flags & PF_KTHREAD) &&
+		!test_bit(MMF_DOVETAILED, &mm->flags)) {
+		mmap_write_lock(mm);
+		__set_bit(MMF_DOVETAILED, &mm->flags);
+		mmap_write_unlock(mm);
+	}
 }
 EXPORT_SYMBOL_GPL(dovetail_init_altsched);
 
diff --git a/mm/memory.c b/mm/memory.c
index d6fc798d8..1e1611242 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -805,6 +805,14 @@ copy_present_page(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma
 	if (!is_cow_mapping(src_vma->vm_flags))
 		return 1;
 
+	/*
+	 * If the source mm belongs to a Dovetail-enabled process, we
+	 * don't want to impose the COW-induced latency on it: make
+	 * sure the child gets its own copy of the page.
+	 */
+	if (dovetailing() && test_bit(MMF_DOVETAILED, &src_mm->flags))
+		goto do_copy;
+
 	/*
 	 * What we want to do is to check whether this page may
 	 * have been pinned by the parent process.  If so,
@@ -823,6 +831,7 @@ copy_present_page(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma
 	if (likely(!page_maybe_dma_pinned(page)))
 		return 1;
 
+do_copy:
 	new_page = *prealloc;
 	if (!new_page)
 		return -EAGAIN;
@@ -5313,66 +5322,6 @@ long copy_huge_page_from_user(struct page *dst_page,
 }
 #endif /* CONFIG_TRANSPARENT_HUGEPAGE || CONFIG_HUGETLBFS */
 
-#ifdef CONFIG_DOVETAIL
-
-int commit_vma(struct mm_struct *mm, struct vm_area_struct *vma)
-{
-	unsigned int gup_flags;
-	int ret, npages;
-
-	if (vma->vm_flags & (VM_IO | VM_PFNMAP))
-		return 0;
-
-	if (!((vma->vm_flags & VM_DONTEXPAND) ||
-	    is_vm_hugetlb_page(vma) || vma == get_gate_vma(mm))) {
-		ret = populate_vma_page_range(vma, vma->vm_start, vma->vm_end,
-					      NULL);
-		return ret < 0 ? ret : 0;
-	}
-
-	gup_flags = (vma->vm_flags & (VM_WRITE | VM_SHARED)) == VM_WRITE
-		? FOLL_WRITE : 0;
-	npages = DIV_ROUND_UP(vma->vm_end, PAGE_SIZE) - vma->vm_start/PAGE_SIZE;
-	ret = get_user_pages(vma->vm_start, npages, gup_flags, NULL, NULL);
-	if (ret < 0)
-		return ret;
-
-	return ret == npages ? 0 : -EFAULT;
-}
-
-int force_commit_memory(void)
-{
-	struct task_struct *tsk = current;
-	struct vm_area_struct *vma;
-	struct mm_struct *mm;
-	int ret = 0;
-
-	mm = get_task_mm(tsk);
-	if (!mm)
-		return -EPERM;
-
-	mmap_write_lock(mm);
-	if (test_bit(MMF_VM_PINNED, &mm->flags))
-		goto done_mm;
-
-	for (vma = mm->mmap; vma; vma = vma->vm_next) {
-		if (is_cow_mapping(vma->vm_flags) &&
-		    (vma->vm_flags & VM_WRITE)) {
-			ret = commit_vma(mm, vma);
-			if (ret < 0)
-				goto done_mm;
-		}
-	}
-	set_bit(MMF_VM_PINNED, &mm->flags);
-done_mm:
-	mmap_write_unlock(mm);
-	mmput(mm);
-
-	return ret;
-}
-
-#endif
-
 #if USE_SPLIT_PTE_PTLOCKS && ALLOC_SPLIT_PTLOCKS
 
 static struct kmem_cache *page_ptl_cachep;
diff --git a/mm/mprotect.c b/mm/mprotect.c
index 42cd6f2ba..b1124224b 100644
--- a/mm/mprotect.c
+++ b/mm/mprotect.c
@@ -23,7 +23,6 @@
 #include <linux/swapops.h>
 #include <linux/mmu_notifier.h>
 #include <linux/migrate.h>
-#include <linux/dovetail.h>
 #include <linux/perf_event.h>
 #include <linux/pkeys.h>
 #include <linux/ksm.h>
@@ -359,7 +358,6 @@ unsigned long change_protection(struct vm_area_struct *vma, unsigned long start,
 		       unsigned long cp_flags)
 {
 	unsigned long pages;
-	bool prot_numa = cp_flags & MM_CP_PROT_NUMA;
 
 	BUG_ON((cp_flags & MM_CP_UFFD_WP_ALL) == MM_CP_UFFD_WP_ALL);
 
@@ -369,12 +367,6 @@ unsigned long change_protection(struct vm_area_struct *vma, unsigned long start,
 		pages = change_protection_range(vma, start, end, newprot,
 						cp_flags);
 
-	if (dovetailing() && !prot_numa &&
-	    test_bit(MMF_VM_PINNED, &vma->vm_mm->flags) &&
-	    ((vma->vm_flags | vma->vm_mm->def_flags) & VM_LOCKED) &&
-	    (vma->vm_flags & (VM_READ | VM_WRITE | VM_EXEC)))
-		commit_vma(vma->vm_mm, vma);
-
 	return pages;
 }
 
-- 
2.38.1

